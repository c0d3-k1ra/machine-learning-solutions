{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Generic Binary Classification Template\n",
    "\n",
    "## üìã How to Use This Template\n",
    "\n",
    "1. **Copy this notebook** to your project folder\n",
    "2. **Update the Configuration Section** below with your dataset details\n",
    "3. **Run cells sequentially** and customize as needed\n",
    "4. **Remove/Add sections** based on your specific requirements\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è CONFIGURATION - CUSTOMIZE FOR YOUR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET CONFIGURATION - UPDATE THESE VALUES\n",
    "# ============================================================================\n",
    "\n",
    "# File paths\n",
    "DATASET_PATH = 'data/your_dataset.csv'  # TODO: Update path\n",
    "TARGET_COLUMN = 'target'                # TODO: Target column name\n",
    "\n",
    "# Feature lists (leave empty for auto-detection)\n",
    "NUMERIC_FEATURES = []      # e.g., ['age', 'income', 'score']\n",
    "CATEGORICAL_FEATURES = []  # e.g., ['gender', 'category']\n",
    "FEATURES_TO_DROP = []      # e.g., ['id', 'timestamp']\n",
    "\n",
    "# Features with impossible zeros (will be treated as missing)\n",
    "ZERO_AS_MISSING = []       # e.g., ['blood_pressure', 'glucose']\n",
    "\n",
    "# Model parameters\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.15\n",
    "VALIDATION_SIZE = 0.15\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# Problem description\n",
    "PROBLEM_NAME = \"Binary Classification Problem\"\n",
    "BUSINESS_OBJECTIVE = \"Describe your prediction goal\"\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Rows: {df.shape[0]:,} | Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Initial Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target Distribution:\")\n",
    "print(df[TARGET_COLUMN].value_counts())\n",
    "print(\"\\nPercentage:\")\n",
    "print(df[TARGET_COLUMN].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "df[TARGET_COLUMN].value_counts().plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title(f'Class Distribution - {TARGET_COLUMN}')\n",
    "df[TARGET_COLUMN].value_counts().plot.pie(autopct='%1.1f%%', ax=axes[1])\n",
    "axes[1].set_title('Class Distribution %')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing': missing, 'Percentage': missing_pct})\n",
    "missing_df = missing_df[missing_df['Missing'] > 0].sort_values('Missing', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect features if not specified\n",
    "if not NUMERIC_FEATURES and not CATEGORICAL_FEATURES:\n",
    "    feature_cols = [c for c in df.columns if c != TARGET_COLUMN and c not in FEATURES_TO_DROP]\n",
    "    NUMERIC_FEATURES = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    CATEGORICAL_FEATURES = df[feature_cols].select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric Features ({len(NUMERIC_FEATURES)}): {NUMERIC_FEATURES}\")\n",
    "print(f\"Categorical Features ({len(CATEGORICAL_FEATURES)}): {CATEGORICAL_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "if NUMERIC_FEATURES:\n",
    "    n = len(NUMERIC_FEATURES)\n",
    "    cols = 3\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 4))\n",
    "    axes = axes.ravel() if n > 1 else [axes]\n",
    "    \n",
    "    for i, feat in enumerate(NUMERIC_FEATURES):\n",
    "        df[feat].hist(bins=30, ax=axes[i], edgecolor='black')\n",
    "        axes[i].set_title(feat)\n",
    "    \n",
    "    for i in range(n, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Feature Distributions', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature vs Target analysis\n",
    "if NUMERIC_FEATURES:\n",
    "    n = len(NUMERIC_FEATURES)\n",
    "    cols = 3\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 4))\n",
    "    axes = axes.ravel() if n > 1 else [axes]\n",
    "    \n",
    "    for i, feat in enumerate(NUMERIC_FEATURES):\n",
    "        for label in df[TARGET_COLUMN].unique():\n",
    "            subset = df[df[TARGET_COLUMN] == label]\n",
    "            axes[i].hist(subset[feat], alpha=0.6, label=f'{TARGET_COLUMN}={label}', bins=20)\n",
    "        axes[i].set_title(feat)\n",
    "        axes[i].legend()\n",
    "    \n",
    "    for i in range(n, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'Features vs {TARGET_COLUMN}', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUMERIC_FEATURES:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr = df[NUMERIC_FEATURES + [TARGET_COLUMN]].corr()\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nCorrelation with {TARGET_COLUMN}:\")\n",
    "    print(corr[TARGET_COLUMN].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Handle zeros as missing values\n",
    "if ZERO_AS_MISSING:\n",
    "    for col in ZERO_AS_MISSING:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed.loc[df_processed[col] == 0, col] = np.nan\n",
    "            print(f\"Replaced {(df[col] == 0).sum()} zeros with NaN in {col}\")\n",
    "\n",
    "# Fill missing values with median\n",
    "if df_processed.isnull().sum().sum() > 0:\n",
    "    for col in df_processed.select_dtypes(include=[np.number]).columns:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            median_val = df_processed[col].median()\n",
    "            df_processed[col].fillna(median_val, inplace=True)\n",
    "            print(f\"Filled {col} with median: {median_val:.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "print(f\"Missing values remaining: {df_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom feature engineering here\n",
    "# Example: df_processed['new_feature'] = df_processed['feat1'] / df_processed['feat2']\n",
    "\n",
    "print(\"Feature engineering step - customize as needed\")\n",
    "print(f\"Current features: {df_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_processed.drop([TARGET_COLUMN] + FEATURES_TO_DROP, axis=1, errors='ignore')\n",
    "y = df_processed[TARGET_COLUMN]\n",
    "\n",
    "# First split: separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: separate validation from training\n",
    "val_size_adjusted = VALIDATION_SIZE / (1 - TEST_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=val_size_adjusted, random_state=RANDOM_STATE, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nClass distribution - Train: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Class distribution - Val: {y_val.value_counts().to_dict()}\")\n",
    "print(f\"Class distribution - Test: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Feature scaling complete!\")\n",
    "print(f\"Mean after scaling: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Std after scaling: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "baseline = DummyClassifier(strategy='most_frequent', random_state=RANDOM_STATE)\n",
    "baseline.fit(X_train_scaled, y_train)\n",
    "baseline_acc = baseline.score(X_val_scaled, y_val)\n",
    "\n",
    "print(f\"Baseline Accuracy (most frequent): {baseline_acc:.4f}\")\n",
    "print(\"Models must beat this baseline to be useful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'SVM': SVC(random_state=RANDOM_STATE, probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_val_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    roc = roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    results[name] = {'accuracy': acc, 'roc_auc': roc, 'model': model}\n",
    "    print(f\"{name}: Accuracy={acc:.4f}, ROC-AUC={roc:.4f if roc else 'N/A'}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
    "    'ROC-AUC': [r['roc_auc'] if r['roc_auc'] else 0 for r in results.values()]\n",
    "})\n",
    "results_df = results_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "results_df.plot(x='Model', y='Accuracy', kind='bar', ax=axes[0], legend=False)\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "results_df.plot(x='Model', y='ROC-AUC', kind='bar', ax=axes[1], legend=False, color='orange')\n",
    "axes[1].set_title('Model ROC-AUC Comparison')\n",
    "axes[1].set_ylabel('ROC-AUC')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel Rankings:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£5Ô∏è‚É£ Select Best Model & Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on ROC-AUC\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['roc_auc'] if x[1]['roc_auc'] else 0)[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "print(f\"\\nValidation Performance:\")\n",
    "y_pred = best_model.predict(X_val_scaled)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "if hasattr(best_model, 'predict_proba'):\n",
    "    y_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {best_model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£6Ô∏è‚É£ Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(f\"Cross-Validation ROC-AUC Scores: {cv_scores}\")\n",
    "print(f\"Mean: {cv_scores.mean():.4f}\")\n",
    "print(f\"Std: {cv_scores.std():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(1, CV_FOLDS + 1), cv_scores)\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Mean: {cv_scores.mean():.4f}')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('ROC-AUC Score')\n",
    "plt.title(f'{CV_FOLDS}-Fold Cross-Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£7Ô∏è‚É£ Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance['Feature'], importance['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop Features:\")\n",
    "    print(importance.head(10))\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': best_model.coef_[0]\n",
    "    }).sort_values('Coefficient', ascending=False, key=abs)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if x < 0 else 'green' for x in importance['Coefficient']]\n",
    "    plt.barh(importance['Feature'], importance['Coefficient'], color=colors)\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.title(f'Feature Coefficients - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop Features:\")\n",
    "    print(importance.head(10))\n",
    "else:\n",
    "    print(\"Feature importance not available for this model type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£8Ô∏è‚É£ Handle Class Imbalance (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "class_counts = y_train.value_counts()\n",
    "imbalance_ratio = class_counts.iloc[0] / class_counts.iloc[1]\n",
    "\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(f\"‚ö†Ô∏è Class imbalance detected: {imbalance_ratio:.2f}:1\")\n",
    "    print(\"Applying SMOTE...\")\n",
    "    \n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    # Retrain best model\n",
    "    best_model_balanced = type(best_model)(**best_model.get_params())\n",
    "    best_model_balanced.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    y_pred_balanced = best_model_balanced.predict(X_val_scaled)\n",
    "    print(\"\\nBalanced Model Performance:\")\n",
    "    print(classification_report(y_val, y_pred_balanced))\n",
    "    \n",
    "    # Compare\n",
    "    roc_original = roc_auc_score(y_val, best_model.predict_proba(X_val_scaled)[:, 1])\n",
    "    roc_balanced = roc_auc_score(y_val, best_model_balanced.predict_proba(X_val_scaled)[:, 1])\n",
    "    \n",
    "    print(f\"\\nROC-AUC Original: {roc_original:.4f}\")\n",
    "    print(f\"ROC-AUC Balanced: {roc_balanced:.4f}\")\n",
    "    \n",
    "    if roc_balanced > roc_original:\n",
    "        print(\"‚úÖ Using balanced model!\")\n",
    "        best_model = best_model_balanced\n",
    "else:\n",
    "    print(\"‚úÖ Classes are reasonably balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£9Ô∏è‚É£ Final Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Final Test Set Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "y_test_proba = best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "print(f\"\\nModel: {best_model_name}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "if y_test_proba is not None:\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    print(f\"\\nTest ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Test Set Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£0Ô∏è‚É£ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and scaler\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "print(\"‚úÖ Model saved as 'best_model.pkl'\")\n",
    "print(\"‚úÖ Scaler saved as 'scaler.pkl'\")\n",
    "\n",
    "# Save feature names for future use\n",
    "feature_names = X.columns.tolist()\n",
    "joblib.dump(feature_names, 'feature_names.pkl')\n",
    "print(\"‚úÖ Feature names saved as 'feature_names.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£1Ô∏è‚É£ Conclusion\n",
    "\n",
    "### Summary\n",
    "- **Best Model:** {best_model_name}\n",
    "- **Test Accuracy:** {Check above}\n",
    "- **Test ROC-AUC:** {Check above}\n",
    "\n",
    "### Next Steps\n",
    "1. Deploy model to production\n",
    "2. Monitor model performance over time\n",
    "3. Retrain with new data periodically\n",
    "4. Consider ensemble methods for improvement\n",
    "5. Implement explainability tools (SHAP, LIME)\n",
    "\n",
    "### How to Use Saved Model\n",
    "```python\n",
    "# Load model\n",
    "model = joblib.load('best_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "feature_names = joblib.load('feature_names.pkl')\n",
    "\n",
    "# Make predictions\n",
    "new_data = pd.DataFrame(...)  # Your new data\n",
    "new_data_scaled = scaler.transform(new_data[feature_names])\n",
    "predictions = model.predict(new_data_scaled)\n",
    "probabilities = model.predict_proba(new_data_scaled)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
